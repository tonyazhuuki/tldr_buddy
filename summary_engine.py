#!/usr/bin/env python3
"""
Two-Mode Summary Engine (TLDRBuddy)
Handles CHAT and LONGFORM summarization modes with automatic routing
"""

import asyncio
import json
import logging
import os
import re
import time
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Any, Union
from pathlib import Path

import openai
from openai import OpenAI

logger = logging.getLogger(__name__)


class SummaryMode(Enum):
    """Summary processing modes"""
    CHAT = "chat"
    LONGFORM = "longform"


class ContentType(Enum):
    """Content source types for routing"""
    TELEGRAM_VOICE = "telegram_voice"
    TELEGRAM_VIDEO_NOTE = "telegram_video_note"
    TELEGRAM_AUDIO = "telegram_audio"
    TELEGRAM_DOCUMENT = "telegram_document"
    TELEGRAM_VIDEO = "telegram_video"
    UPLOADED_URL = "uploaded_url"
    TEXT_INPUT = "text_input"


@dataclass
class SummaryConfig:
    """Configuration for summary processing"""
    mode: SummaryMode
    model: str
    max_tokens: int
    temperature: float
    frequency_penalty: float
    reasoning_effort: Optional[str] = None


@dataclass
class SummaryResult:
    """Result of summary processing"""
    success: bool
    summary: Optional[str] = None
    mode: Optional[SummaryMode] = None
    processing_time: Optional[float] = None
    token_count: Optional[int] = None
    error_message: Optional[str] = None
    confidence: Optional[float] = None


class SummaryEngine:
    """
    Two-mode summary engine for TLDRBuddy
    Handles automatic routing between CHAT and LONGFORM modes
    """
    
    def __init__(self, openai_client: Optional[OpenAI] = None):
        self.client = openai_client
        
        # Default configurations for each mode
        self.configs = {
            SummaryMode.CHAT: SummaryConfig(
                mode=SummaryMode.CHAT,
                model="gpt-5-mini",
                max_tokens=1100,
                temperature=0.25,
                frequency_penalty=0.2,
                reasoning_effort="minimal"
            ),
            SummaryMode.LONGFORM: SummaryConfig(
                mode=SummaryMode.LONGFORM,
                model="gpt-5-mini",
                max_tokens=1500,
                temperature=0.25,
                frequency_penalty=0.2,
                reasoning_effort="minimal"
            )
        }
        
        # Content type to mode routing
        self.content_routing = {
            ContentType.TELEGRAM_VOICE: SummaryMode.CHAT,
            ContentType.TELEGRAM_VIDEO_NOTE: SummaryMode.CHAT,
            ContentType.TELEGRAM_AUDIO: SummaryMode.CHAT,
            ContentType.TELEGRAM_DOCUMENT: SummaryMode.LONGFORM,
            ContentType.TELEGRAM_VIDEO: SummaryMode.LONGFORM,
            ContentType.UPLOADED_URL: SummaryMode.LONGFORM,
            ContentType.TEXT_INPUT: SummaryMode.CHAT  # Default for text
        }
        
        # System prompts for each mode
        self.system_prompts = {
            SummaryMode.CHAT: self._get_chat_system_prompt(),
            SummaryMode.LONGFORM: self._get_longform_system_prompt()
        }
        
        # Feature flag for enabling new functionality
        self.enabled = os.getenv('TLDRBUDDY_ENABLED', 'false').lower() == 'true'
        
        logger.info(f"SummaryEngine initialized, enabled: {self.enabled}")
    
    def _get_chat_system_prompt(self) -> str:
        """Get system prompt for CHAT mode"""
        from datetime import datetime
        import pytz
        
        # Get current date in Europe/Amsterdam timezone
        amsterdam_tz = pytz.timezone('Europe/Amsterdam')
        today_iso = datetime.now(amsterdam_tz).strftime('%Y-%m-%d')
        
        return f"""Ð¢Ñ‹ â€” TLDRBuddy. ÐŸÐ¸ÑˆÐ¸ Ð½Ð° ÑÐ·Ñ‹ÐºÐµ Ð²Ñ…Ð¾Ð´Ð° (ÐµÑÐ»Ð¸ ÑÐ¼ÐµÑÑŒ â€” Ñ€ÑƒÑÑÐºÐ¸Ð¹). ÐÐµ Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ñ„Ð°ÐºÑ‚Ñ‹ Ð¸ ÑÑÑ‹Ð»ÐºÐ¸.
Ð•ÑÐ»Ð¸ Ð²Ñ…Ð¾Ð´ Ð¿ÑƒÑÑ‚/ÑˆÑƒÐ¼/Ð¼ÑƒÐ·Ñ‹ÐºÐ° Ð±ÐµÐ· Ñ€ÐµÑ‡Ð¸ â€” Ð²ÐµÑ€Ð½Ð¸: Â«ðŸ§¯ ÐÐ• ÐÐÐÐ›Ð˜Ð—Ð˜Ð Ð£Ð®: [Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð°]Â».

Ð”ÐÐ¢Ð«/Ð§Ð˜Ð¡Ð›Ð:
â€” Ð’ÑÐµÐ³Ð´Ð° Ð²Ñ‹Ð´ÐµÐ»ÑÐ¹ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¼ Ð±Ð»Ð¾ÐºÐ¾Ð¼ Â«Ð”ÐÐÐÐ«Ð•/Ð¦Ð˜Ð¤Ð Ð«Â».
â€” ÐŸÐ¾Ð´ÑÐ²ÐµÑ‡Ð¸Ð²Ð°Ð¹: Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð´Ð°Ñ‚Ñ‹ (Â«ÑÐµÐ³Ð¾Ð´Ð½ÑÂ», Â«Ð·Ð°Ð²Ñ‚Ñ€Ð°Â», Â«15-Ð³Ð¾Â»), Ð°Ð±ÑÐ¾Ð»ÑŽÑ‚Ð½Ñ‹Ðµ Ð´Ð°Ñ‚Ñ‹, ÑÑƒÐ¼Ð¼Ñ‹, Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ñ‹, ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð°, Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ñ‹.
â€” ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐ¹ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð´Ð°Ñ‚Ñ‹ Ðº ISO (YYYY-MM-DD), Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¾Ð¿Ð¾Ñ€Ð½ÑƒÑŽ Ð´Ð°Ñ‚Ñƒ {today_iso} Ð¸ Ñ‡Ð°ÑÐ¾Ð²Ð¾Ð¹ Ð¿Ð¾ÑÑ Europe/Amsterdam. 
  ÐŸÑ€Ð¸Ð¼ÐµÑ€: Â«Ð·Ð°Ð²Ñ‚Ñ€Ð°Â» â†’ {today_iso}+1; Â«15-Ð³Ð¾Â» â†’ Ð±Ð»Ð¸Ð¶Ð°Ð¹ÑˆÐµÐµ Ð±ÑƒÐ´ÑƒÑ‰ÐµÐµ 15 Ñ‡Ð¸ÑÐ»Ð¾ (ÐµÑÐ»Ð¸ Ð¼ÐµÑÑÑ† Ð½Ðµ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¸ Ð½ÐµÐ¾Ñ‡ÐµÐ²Ð¸Ð´Ð½Ð¾ â€” Ð¿Ð¾Ð¼ÐµÑ‚ÑŒ Â«Ð¼ÐµÑÑÑ† Ð½Ðµ ÑƒÐºÐ°Ð·Ð°Ð½Â» Ð¸ ÑƒÐºÐ°Ð¶Ð¸ Ð¾Ð±Ðµ Ð²ÐµÑ€ÑÐ¸Ð¸).
â€” Ð•ÑÐ»Ð¸ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð²ÑƒÑÐ¼Ñ‹ÑÐ»ÐµÐ½Ð½Ð° â€” Ð¾Ñ‚Ð´Ð°Ð¹ Ð¸ ÑÑ‹Ñ€Ð¾Ðµ, Ð¸ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ (Ñ Ð¿Ð¾Ð¼ÐµÑ‚ÐºÐ¾Ð¹ Â«Ð½ÐµÐ¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ð¾Â»).

Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð:
â€” ÐÐµ Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐ¹ Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÐµÐºÑ†Ð¸Ð¸. Ð•ÑÐ»Ð¸ Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð½ÐµÑ‚ â€” Ð¿Ð¸ÑˆÐ¸ Â«Ð½ÐµÑ‚Â».
â€” Ð—Ð°Ð¿Ñ€ÐµÑ‰ÐµÐ½Ñ‹ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ñ„Ñ€Ð°Ð·Ñ‹ Ñ‚Ð¸Ð¿Ð° Â«ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹ Ð²Ñ‹Ð´ÐµÐ»ÐµÐ½Ñ‹Â».
â€” Ð’ Â«Ð”Ð•Ð™Ð¡Ð¢Ð’Ð˜Ð¯Ð¥Â» Ð½Ðµ Ð¿Ñ€Ð¸Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸. Ð•ÑÐ»Ð¸ ÑÐ²Ð½Ñ‹Ñ… Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð½ÐµÑ‚ â€” Ð½Ð°Ð¿Ð¸ÑˆÐ¸ Â«Ð½ÐµÑ‚Â».

ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ñ‚ÐµÐºÑÑ‚/Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ‚ Ð½Ð¸Ð¶Ðµ. ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ Ð¸ Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ð½Ð¾. Ð¯Ð·Ñ‹Ðº Ð¾Ñ‚Ð²ÐµÑ‚Ð° = ÑÐ·Ñ‹Ðº Ð²Ñ…Ð¾Ð´Ð°.
ÐžÐ¿Ð¾Ñ€Ð½Ð°Ñ Ð´Ð°Ñ‚Ð°: {today_iso} (Europe/Amsterdam).

Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚:
ðŸ“ Ð Ð•Ð—Ð®ÐœÐ•: 1â€“2 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ â€” ÑÑƒÑ‚ÑŒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ
ðŸ“ ÐžÐ¡ÐÐžÐ’ÐÐ«Ð• ÐŸÐ£ÐÐšÐ¢Ð« (3â€“7): â€¢ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ Ñ„Ð°ÐºÑ‚/Ð½Ð°Ð¼ÐµÑ€ÐµÐ½Ð¸Ðµ/Ð´Ð¾Ð³Ð¾Ð²Ð¾Ñ€Ñ‘Ð½Ð½Ð¾ÑÑ‚ÑŒ
ðŸ“Š Ð”ÐÐÐÐ«Ð•/Ð¦Ð˜Ð¤Ð Ð«:
â€¢ [Ñ‚Ð¸Ð¿] â€” [Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ] â€” [ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚/Ñ†Ð¸Ñ‚Ð°Ñ‚Ð°]
âš¡ Ð”Ð•Ð™Ð¡Ð¢Ð’Ð˜Ð¯: 
â€¢ [Ð¸Ð¼Ð¿ÐµÑ€Ð°Ñ‚Ð¸Ð², Ð¸Ð·Ð¼ÐµÑ€Ð¸Ð¼Ð¾] [â€” Ð²Ð»Ð°Ð´ÐµÐ»ÐµÑ†/Ñ€Ð¾Ð»ÑŒ] [â€” ÑÑ€Ð¾Ðº] [â€” P1|P2|P3]
Ð•ÑÐ»Ð¸ ÑÐ²Ð½Ñ‹Ñ… Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð½ÐµÑ‚ â€” Â«Ð½ÐµÑ‚Â»
ðŸŽ­ Ð¢ÐžÐ/Ð­ÐœÐžÐ¦Ð˜Ð˜: 1â€“3 ÑÐ»Ð¾Ð²Ð° (Ð½Ð°Ð¿Ñ€.: Ñ‚Ñ‘Ð¿Ð»Ñ‹Ð¹, ÑƒÑÑ‚Ð°Ð»Ð¾ÑÑ‚ÑŒ, Ð²Ð¾Ð¾Ð´ÑƒÑˆÐµÐ²Ð»ÐµÐ½Ð¸Ðµ)

ÐŸÑ€Ð°Ð²Ð¸Ð»Ð°:
- Ð£Ð±Ð¸Ñ€Ð°Ð¹ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ñ‹, Ð½Ðµ Ð¿Ð¸ÑˆÐ¸ Ð¾Ð±Ñ‰Ð¸Ðµ ÑÐ»Ð¾Ð²Ð°.
- Ð’ Â«ÐžÐ¡ÐÐžÐ’ÐÐ«Ð¥ ÐŸÐ£ÐÐšÐ¢ÐÐ¥Â» Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÐµÐ½ ÑÐ¼Ñ‹ÑÐ»: ÐºÑ‚Ð¾/Ñ‡Ñ‚Ð¾/ÐºÐ¾Ð³Ð´Ð°; Ð±ÐµÐ· Â«Ð²Ð¾Ð´Ð°/Ð¿Ñ€Ð¾ Ð²Ð¸Ð´Â».
- Ð”Ð°Ñ‚Ñ‹/Ñ‡Ð¸ÑÐ»Ð° Ð²ÑÐµÐ³Ð´Ð° Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÐ¹ Ð² Â«Ð”ÐÐÐÐ«Ð•/Ð¦Ð˜Ð¤Ð Ð«Â» (Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹, ÑÐ¼. SYSTEM).
- Ð¢Ð°Ð¹Ð¼ÐºÐ¾Ð´Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ ÐµÑÑ‚ÑŒ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ; Ð½Ðµ Ð¿Ñ€Ð¸Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹."""
    
    def _get_longform_system_prompt(self) -> str:
        """Get system prompt for LONGFORM mode"""
        return """Ð¢Ñ‹ â€” TLDRBuddy. ÐÐµ Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ñ„Ð°ÐºÑ‚Ñ‹ Ð¸ ÑÑÑ‹Ð»ÐºÐ¸. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° ÑÐ·Ñ‹ÐºÐµ Ð²Ñ…Ð¾Ð´Ð°; ÐµÑÐ»Ð¸ Ð¾Ð½ ÑÐ¼ÐµÑˆÐ°Ð½Ð½Ñ‹Ð¹ â€” Ñ€ÑƒÑÑÐºÐ¸Ð¹.
Ð•ÑÐ»Ð¸ Ð²Ñ…Ð¾Ð´ Ð¿ÑƒÑÑ‚/ÑˆÑƒÐ¼/Ð¼ÑƒÐ·Ñ‹ÐºÐ° Ð±ÐµÐ· Ñ€ÐµÑ‡Ð¸ â€” Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾Ðµ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ Â«ÐÐ• ÐÐÐÐ›Ð˜Ð—Ð˜Ð Ð£Ð®: [Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð°]Â».
Ð•ÑÐ»Ð¸ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ Ð½ÐµÑ‚ ÑÐ²Ð½Ñ‹Ñ… Â«Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹Â» â€” Ñ‚Ð°Ðº Ð¸ Ð¿Ð¸ÑˆÐ¸, Ñ€Ð°Ð·Ð´ÐµÐ» Ð½Ðµ Ð½Ð°Ð¿Ð¾Ð»Ð½ÑÐ¹ Ð¼ÑƒÑÐ¾Ñ€Ð¾Ð¼.
Ð¦Ð¸Ñ„Ñ€Ñ‹/Ð´Ð°Ð½Ð½Ñ‹Ðµ â€” Ð²ÑÐµÐ³Ð´Ð° Ð²Ñ‹Ð´ÐµÐ»ÑÐ¹ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾ (Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ñ‹, ÑÑƒÐ¼Ð¼Ñ‹, ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð°, Ð´Ð°Ñ‚Ñ‹, Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸). ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐ¹ ÐµÐ´Ð¸Ð½Ð¸Ñ†Ñ‹, Ð½Ð¾ Ð½Ðµ Ð¿Ñ€Ð¸Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹.
Ð¢Ð°Ð¹Ð¼ÐºÐ¾Ð´Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÑÐ²Ð½Ð¾ ÐµÑÑ‚ÑŒ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ/Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ‚Ðµ. ÐÐµ Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ñ‚Ð°Ð¹Ð¼ÐºÐ¾Ð´Ñ‹.
Ð’ JSON-Ñ€ÐµÐ¶Ð¸Ð¼Ð°Ñ… â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ JSON Ð¿Ð¾ ÑÑ…ÐµÐ¼Ðµ, Ð½Ð¸Ñ‡ÐµÐ³Ð¾ ÑÐ½Ð°Ñ€ÑƒÐ¶Ð¸.

ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð» (Ð»ÐµÐºÑ†Ð¸Ñ/Ð¿Ð¾Ð´ÐºÐ°ÑÑ‚/Ð¸Ð½Ñ‚ÐµÑ€Ð²ÑŒÑŽ/Ñ€Ð°Ð½Ð´Ð¾Ð¼Ð½Ð¾Ðµ Ð°ÑƒÐ´Ð¸Ð¾-Ð²Ð¸Ð´ÐµÐ¾). ÐÐµ Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹. Ð¯Ð·Ñ‹Ðº Ð¾Ñ‚Ð²ÐµÑ‚Ð° = ÑÐ·Ñ‹Ðº Ð²Ñ…Ð¾Ð´Ð°.

ðŸ§  **Ð¢Ð•Ð—Ð˜Ð¡**: 1â€“2 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ â€” ÑÑƒÑ‚ÑŒ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°
ðŸ”‘ **ÐšÐ›Ð®Ð§Ð•Ð’Ð«Ð• Ð˜Ð”Ð•Ð˜ (5â€“9)**: â€¢ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾, Ð±ÐµÐ· Ð²Ð¾Ð´Ñ‹
ðŸ—ºï¸ **Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð/Ð¡Ð•Ð“ÐœÐ•ÐÐ¢Ð«**:
â€¢ [ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°] â€” 1 ÑÑ‚Ñ€Ð¾ÐºÐ° ÑÐ¼Ñ‹ÑÐ»Ð°
(Ð¢Ð°Ð¹Ð¼ÐºÐ¾Ð´Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ; ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ â€” Ð½Ðµ Ð¿Ñ€Ð¸Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹.)
âš–ï¸ **ÐÐ Ð“Ð£ÐœÐ•ÐÐ¢Ð« â†” Ð’ÐžÐ—Ð ÐÐ–Ð•ÐÐ˜Ð¯**:
â€¢ Ñ‚ÐµÐ·Ð¸Ñ â†’ Ñ„Ð°ÐºÑ‚Ñ‹/Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°
â€¢ ÐºÐ¾Ð½Ñ‚Ñ€Ñ‚ÐµÐ·Ð¸Ñ (ÐµÑÐ»Ð¸ Ð·Ð²ÑƒÑ‡Ð¸Ñ‚) â†’ Ñ„Ð°ÐºÑ‚Ñ‹/Ð¿Ñ€Ð¸Ð¼ÐµÑ€
ðŸ“Š **Ð”ÐÐÐÐ«Ð•/Ð¦Ð˜Ð¤Ð Ð«**:
â€¢ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ° â€” Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ â€” ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ (Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ñ‹, ÑÑƒÐ¼Ð¼Ñ‹, ÐºÐ¾Ð»-Ð²Ð°, Ð´Ð°Ñ‚Ñ‹, ÑˆÐ°Ð³Ð¸/ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸/Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ñ‹)
ðŸ“š **Ð“Ð›ÐžÐ¡Ð¡ÐÐ Ð˜Ð™ (Ð´Ð¾ 10)**: Â«Ñ‚ÐµÑ€Ð¼Ð¸Ð½ â€” Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ðµ Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚ÑƒÂ»
ðŸ’¬ **Ð¦Ð˜Ð¢ÐÐ¢Ð« (3â€“7)**: Â«ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ°Ñ Ñ‚Ð¾Ñ‡Ð½Ð°Ñ Ñ†Ð¸Ñ‚Ð°Ñ‚Ð°Â»
ðŸ§© **ÐÐ•Ð¯Ð¡ÐÐž**: Ð²Ð°Ð¶Ð½Ñ‹Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð½Ð° ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð» Ð½Ðµ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚
ðŸŽ› **Ð£Ð’Ð•Ð Ð•ÐÐÐžÐ¡Ð¢Ð¬**: 0.0â€“1.0"""
    
    def determine_mode(self, 
                      content_type: ContentType, 
                      text: Optional[str] = None,
                      duration: Optional[int] = None) -> SummaryMode:
        """
        Determine summary mode based on content type and heuristics
        
        Args:
            content_type: Type of content source
            text: Transcribed text for heuristics
            duration: Duration in seconds for heuristics
            
        Returns:
            SummaryMode to use
        """
        # Primary routing by content type
        primary_mode = self.content_routing.get(content_type, SummaryMode.CHAT)
        
        # Apply heuristics if we have text or duration
        if text or duration:
            # Duration heuristic: > 10 minutes â†’ LONGFORM
            if duration and duration > 600:  # 10 minutes
                logger.info(f"Duration heuristic: {duration}s > 600s, switching to LONGFORM")
                return SummaryMode.LONGFORM
            
            # Text length heuristic: > 1500 words â†’ LONGFORM
            if text:
                word_count = len(text.split())
                if word_count > 1500:
                    logger.info(f"Text length heuristic: {word_count} words > 1500, switching to LONGFORM")
                    return SummaryMode.LONGFORM
                
                # Dialog style heuristic: if many "Ñ/Ñ‚Ñ‹/Ð¼Ñ‹" and from URL â†’ can force CHAT
                if content_type == ContentType.UPLOADED_URL:
                    dialog_indicators = len(re.findall(r'\b(Ñ|Ñ‚Ñ‹|Ð¼Ñ‹|Ð²Ñ‹|Ð¾Ð½|Ð¾Ð½Ð°|Ð¾Ð½Ð¸)\b', text, re.IGNORECASE))
                    if dialog_indicators > 10:  # High dialog indicator
                        logger.info(f"Dialog style heuristic: {dialog_indicators} dialog indicators, keeping CHAT")
                        return SummaryMode.CHAT
        
        logger.info(f"Using primary mode {primary_mode} for content type {content_type}")
        return primary_mode
    
    async def process_summary(self, 
                            text: str,
                            content_type: ContentType,
                            duration: Optional[int] = None,
                            force_mode: Optional[SummaryMode] = None) -> SummaryResult:
        """
        Process text through appropriate summary mode
        
        Args:
            text: Text to summarize
            content_type: Type of content source
            duration: Duration in seconds (for heuristics)
            force_mode: Force specific mode (for testing)
            
        Returns:
            SummaryResult with processing results
        """
        if not self.enabled:
            logger.warning("SummaryEngine is disabled, returning error")
            return SummaryResult(
                success=False,
                error_message="SummaryEngine is disabled"
            )
        
        if not self.client:
            logger.warning("OpenAI client not available, returning error")
            return SummaryResult(
                success=False,
                error_message="OpenAI client not available"
            )
        
        start_time = time.time()
        
        try:
            # Determine mode
            if force_mode:
                mode = force_mode
                logger.info(f"Using forced mode: {mode}")
            else:
                mode = self.determine_mode(content_type, text, duration)
            
            # Get configuration for mode
            config = self.configs[mode]
            system_prompt = self.system_prompts[mode]
            
            logger.info(f"Processing summary in {mode} mode with {config.model}")
            
            # Prepare messages
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ÐœÐ°Ñ‚ÐµÑ€Ð¸Ð°Ð»:\n{text}"}
            ]
            
            # Prepare parameters
            params = {
                "model": config.model,
                "messages": messages,
                "max_completion_tokens": config.max_tokens
            }
            
            # Add optional parameters only if they're supported
            # Note: temperature and frequency_penalty may not be supported by all models
            # Add reasoning effort if specified
            if config.reasoning_effort:
                params["reasoning_effort"] = config.reasoning_effort
            
            # Call OpenAI API
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                **params
            )
            
            # Extract result
            summary = response.choices[0].message.content
            token_count = response.usage.total_tokens if response.usage else None
            
            processing_time = time.time() - start_time
            
            logger.info(f"Summary completed in {processing_time:.2f}s, "
                       f"tokens: {token_count}, mode: {mode}")
            
            return SummaryResult(
                success=True,
                summary=summary,
                mode=mode,
                processing_time=processing_time,
                token_count=token_count,
                confidence=0.9  # Default confidence
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Summary processing failed: {e}")
            
            return SummaryResult(
                success=False,
                error_message=str(e),
                processing_time=processing_time
            )
    
    def get_fallback_response(self, text: str) -> str:
        """
        Get fallback response when processing fails
        
        Args:
            text: Original text
            
        Returns:
            Fallback response
        """
        # Check if text is empty/noise/music
        if not text or len(text.strip()) < 10:
            return "ðŸ§¯ ÐÐ• ÐÐÐÐ›Ð˜Ð—Ð˜Ð Ð£Ð®: ÐŸÑƒÑÑ‚Ð¾Ð¹ Ð¸Ð»Ð¸ Ð½ÐµÑ€Ð°Ð·Ð±Ð¾Ñ€Ñ‡Ð¸Ð²Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚"
        
        # Check for noise indicators
        noise_indicators = ["ÑˆÑƒÐ¼", "Ð¼ÑƒÐ·Ñ‹ÐºÐ°", "Ð·Ð²ÑƒÐº", "noise", "music", "sound"]
        text_lower = text.lower()
        if any(indicator in text_lower for indicator in noise_indicators):
            return "ðŸ§¯ ÐÐ• ÐÐÐÐ›Ð˜Ð—Ð˜Ð Ð£Ð®: ÐœÑƒÐ·Ñ‹ÐºÐ° Ð¸Ð»Ð¸ ÑˆÑƒÐ¼ Ð±ÐµÐ· Ñ€ÐµÑ‡Ð¸"
        
        # Default fallback
        return f"""ðŸ§¯ ÐÐ• ÐÐÐÐ›Ð˜Ð—Ð˜Ð Ð£Ð®: ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸

ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚:
{text[:200]}{'...' if len(text) > 200 else ''}"""
    
    def update_config(self, mode: SummaryMode, **kwargs) -> None:
        """
        Update configuration for a specific mode
        
        Args:
            mode: Mode to update
            **kwargs: Configuration parameters to update
        """
        if mode not in self.configs:
            logger.error(f"Unknown mode: {mode}")
            return
        
        config = self.configs[mode]
        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)
                logger.info(f"Updated {mode} config: {key} = {value}")
            else:
                logger.warning(f"Unknown config parameter: {key}")
    
    def enable(self) -> None:
        """Enable the summary engine"""
        self.enabled = True
        logger.info("SummaryEngine enabled")
    
    def disable(self) -> None:
        """Disable the summary engine"""
        self.enabled = False
        logger.info("SummaryEngine disabled")


# Factory function for creating SummaryEngine
def create_summary_engine(openai_client: Optional[OpenAI] = None) -> SummaryEngine:
    """
    Create and configure SummaryEngine instance
    
    Args:
        openai_client: OpenAI client instance
        
    Returns:
        Configured SummaryEngine instance
    """
    return SummaryEngine(openai_client) 